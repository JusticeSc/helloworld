# æ·±å…¥Pythonæ•°æ®åˆ†æå·¥å…·Pandas

---

## ä¸€ã€ç†è§£åº•å±‚ï¼šPandasçš„æ¶æ„å“²å­¦ä¸NumPyåŸºå› 

### 1.1 ä»Excelåˆ°Pandasï¼šä¸¤ç§ä¸åŒçš„è®¾è®¡å“²å­¦

| ç»´åº¦         | Excelï¼ˆä¼ ç»Ÿç”µå­è¡¨æ ¼ï¼‰ | Pandasï¼ˆç°ä»£æ•°æ®åˆ†æåº“ï¼‰ |
| ------------ | --------------------- | ------------------------ |
| **æ•°æ®æ¨¡å‹** | äºŒç»´å•å…ƒæ ¼ç½‘æ ¼        | å¸¦æ ‡ç­¾çš„å¤šç»´æ•°ç»„         |
| **è®¡ç®—èŒƒå¼** | å•å…ƒæ ¼å…¬å¼ï¼Œé€æ ¼è®¡ç®—  | **å‘é‡åŒ–è¿ç®—**ï¼Œæ•´åˆ—æ“ä½œ |
| **å†…å­˜å¸ƒå±€** | åˆ†æ•£å­˜å‚¨ï¼Œé«˜å¼€é”€      | è¿ç»­å†…å­˜å—ï¼Œä½å¼€é”€       |
| **æ‰©å±•æ€§**   | å•æœºã€å•çº¿ç¨‹          | åŸç”Ÿæ”¯æŒå¹¶è¡Œã€é›†ç¾¤æ‰©å±•   |

**æ ¸å¿ƒåŸç†**ï¼šPandasçš„DataFrameæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**å…ƒæ•°æ®å±‚ + NumPyæ•°ç»„å®¹å™¨**ã€‚
```python
# æ·±å…¥æŸ¥çœ‹åº•å±‚æ•°æ®ç»“æ„
import pandas as pd
import numpy as np

# åˆ›å»ºç¤ºä¾‹DataFrame
df = pd.DataFrame({
    'A': np.arange(1_000_000, dtype='int32'),  # 4å­—èŠ‚/å…ƒç´ 
    'B': np.random.randn(1_000_000),          # 8å­—èŠ‚/å…ƒç´ 
    'C': pd.date_range('2023-01-01', periods=1_000_000, freq='T')
})

# å…³é”®æ´å¯Ÿï¼šDataFrameæ˜¯åˆ—å¼å­˜å‚¨
print(f"å†…å­˜å ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print(f"åˆ—'A'çš„NumPyæ•°ç»„ç±»å‹: {type(df['A'].values)}")
print(f"åˆ—'A'çš„NumPyæ•°ç»„dtype: {df['A'].values.dtype}")
```

**ä¸ºä»€ä¹ˆå‘é‡åŒ–å¿«100å€ï¼Ÿ**
```python
# åä¾‹ï¼šPythonçº§å¾ªç¯ï¼ˆææ…¢ï¼ï¼‰
slow_result = []
for val in df['A']:
    slow_result.append(val * 2)

# æ­£ä¾‹ï¼šPandaså‘é‡åŒ–ï¼ˆç¬é—´å®Œæˆï¼‰
fast_result = df['A'] * 2

# æ­£ä¾‹ï¼šNumPyåº•å±‚æ“ä½œï¼ˆæœ€å¿«ï¼‰
fastest_result = df['A'].values * 2

# æ€§èƒ½å¯¹æ¯”ï¼ˆæ¦‚å¿µæ€§ï¼‰ï¼š
# å¾ªç¯: O(n) Ã— Pythonè§£é‡Šå™¨å¼€é”€ â‰ˆ 10-100å€æ…¢
# å‘é‡åŒ–: O(n) Ã— Cçº§ä¼˜åŒ– â‰ˆ åŸç”Ÿé€Ÿåº¦
```

### 1.2 Seriesçš„æ·±å…¥å‰–æï¼šä¸åªæ˜¯â€œå¸¦æ ‡ç­¾çš„æ•°ç»„â€

ä¸€ä¸ªå¸¸è§çš„è¯¯è§£æ˜¯Series = åˆ—è¡¨ + æ ‡ç­¾ã€‚å®é™…ä¸Šï¼ŒSeriesçš„è®¾è®¡ç²¾å¦™å¾—å¤šï¼š

```python
# åˆ›å»ºSeriesçš„å¤šç§æ–¹å¼åŠå…¶å†…å­˜å«ä¹‰
# æ–¹å¼1ï¼šä»åˆ—è¡¨åˆ›å»ºï¼ˆè‡ªåŠ¨ç”ŸæˆRangeIndexï¼‰
s1 = pd.Series([10, 20, 30, 40])
print(f"s1.indexç±»å‹: {type(s1.index)}")  # RangeIndexï¼šå‡ ä¹é›¶å¼€é”€

# æ–¹å¼2ï¼šä»å­—å…¸åˆ›å»ºï¼ˆæ˜¾å¼æ ‡ç­¾ï¼‰
s2 = pd.Series({'a': 10, 'b': 20, 'c': 30, 'd': 40})
print(f"s2.indexç±»å‹: {type(s2.index)}")  # Indexï¼šå­˜å‚¨å“ˆå¸Œæ˜ å°„ï¼Œæ”¯æŒå¿«é€ŸæŸ¥æ‰¾

# æ–¹å¼3ï¼šä»NumPyæ•°ç»„åˆ›å»ºï¼ˆå…±äº«å†…å­˜ï¼‰
arr = np.array([10, 20, 30, 40], dtype='float32')
s3 = pd.Series(arr, index=['x', 'y', 'z', 'w'])
print(f"s3.valuesä¸arrå…±äº«å†…å­˜: {s3.values is arr}")  # Trueï¼šé›¶æ‹·è´ï¼

# Seriesçš„é”®èƒ½åŠ›ï¼šè‡ªåŠ¨å¯¹é½ï¼ˆAuto-alignmentï¼‰
s_left = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
s_right = pd.Series([10, 20, 30], index=['b', 'c', 'd'])

# é­”æ³•æ—¶åˆ»ï¼šç´¢å¼•è‡ªåŠ¨å¯¹é½ï¼Œè€Œä¸æ˜¯ä½ç½®åŒ¹é…
result = s_left + s_right
print("è‡ªåŠ¨å¯¹é½ç»“æœ:")
print(result)
# a     NaN  # ä»…å·¦æœ‰ â†’ NaN
# b    12.0  # å·¦å³éƒ½æœ‰ â†’ 1+10
# c    23.0  # å·¦å³éƒ½æœ‰ â†’ 2+20
# d     NaN  # ä»…å³æœ‰ â†’ NaN
```

---

## äºŒã€DataFrameï¼šç†è§£â€œåˆ—çš„å­—å…¸â€è®¾è®¡èŒƒå¼

### 2.1 DataFrameçš„ä¸‰ç§åˆ›å»ºæ–¹å¼ä¸å†…å­˜å¸ƒå±€

```python
# æ–¹æ³•1ï¼šå­—å…¸å¼åˆ›å»ºï¼ˆæœ€ç›´è§‚ï¼‰
df_dict = pd.DataFrame({
    'employee_id': [101, 102, 103, 104],
    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'salary': [75000, 82000, 68000, 91000],
    'join_date': pd.to_datetime(['2020-03-15', '2019-11-20', '2021-01-10', '2018-07-30'])
})
print("DataFrameç»“æ„é¢„è§ˆ:")
print(df_dict)
print(f"åˆ—ç±»å‹: {df_dict.dtypes.to_dict()}")

# æ–¹æ³•2ï¼šåˆ—è¡¨çš„åˆ—è¡¨ï¼ˆç±»ä¼¼NumPyï¼‰
data = [
    [101, 'Alice', 75000, '2020-03-15'],
    [102, 'Bob', 82000, '2019-11-20'],
    [103, 'Charlie', 68000, '2021-01-10'],
    [104, 'Diana', 91000, '2018-07-30']
]
df_list = pd.DataFrame(data, columns=['employee_id', 'name', 'salary', 'join_date'])

# æ–¹æ³•3ï¼šä»NumPyç»“æ„åŒ–æ•°ç»„ï¼ˆæœ€é«˜æ•ˆï¼‰
dtype = [('employee_id', 'int32'), ('name', 'U20'), ('salary', 'float64'), ('join_date', 'datetime64[s]')]
structured_array = np.array([
    (101, 'Alice', 75000.0, np.datetime64('2020-03-15')),
    (102, 'Bob', 82000.0, np.datetime64('2019-11-20')),
    (103, 'Charlie', 68000.0, np.datetime64('2021-01-10')),
    (104, 'Diana', 91000.0, np.datetime64('2018-07-30'))
], dtype=dtype)
df_structured = pd.DataFrame.from_records(structured_array)
```

### 2.2 å±æ€§è¯¦è§£ï¼š`.shape`, `.columns`, `.dtypes`, `.info()`

```python
# å®æˆ˜ï¼šå¿«é€Ÿæ•°æ®è´¨é‡è¯Šæ–­å·¥ä½œæµ
def data_health_check(df):
    """å…¨é¢çš„æ•°æ®å¥åº·æ£€æŸ¥"""
    print("=" * 60)
    print("DATA HEALTH CHECK REPORT")
    print("=" * 60)

    # 1. åŸºæœ¬ç»´åº¦
    print(f"ğŸ“Š æ•°æ®è§„æ¨¡: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")

    # 2. å†…å­˜ä½¿ç”¨
    mem_usage = df.memory_usage(deep=True)
    print(f"ğŸ’¾ å†…å­˜å ç”¨: {mem_usage.sum() / 1024**2:.2f} MB")
    print("   å„åˆ—è¯¦æƒ…:")
    for col, usage in mem_usage.items():
        print(f"     - {col}: {usage / 1024:.1f} KB")

    # 3. ç±»å‹åˆ†æï¼ˆå…³é”®ï¼ï¼‰
    print("ğŸ”§ æ•°æ®ç±»å‹åˆ†å¸ƒ:")
    type_counts = df.dtypes.value_counts()
    for dtype, count in type_counts.items():
        print(f"     - {dtype}: {count} åˆ—")

    # 4. ç¼ºå¤±å€¼ç»Ÿè®¡
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    print("âš ï¸  ç¼ºå¤±å€¼åˆ†æ:")
    for col in missing[missing > 0].index:
        print(f"     - {col}: {missing[col]} ä¸ª ({missing_pct[col]}%)")

    # 5. å”¯ä¸€å€¼ç»Ÿè®¡ï¼ˆè¯†åˆ«ä½åŸºæ•°åˆ†ç±»ç‰¹å¾ï¼‰
    print("ğŸ¯ å”¯ä¸€å€¼åˆ†æ:")
    for col in df.select_dtypes(include=['object']).columns:
        unique_count = df[col].nunique()
        if unique_count < 20:  # ä½åŸºæ•°ç‰¹å¾
            print(f"     - {col}: {unique_count} ä¸ªå”¯ä¸€å€¼")
            print(f"       æ ·æœ¬: {df[col].unique()[:5]}...")

    print("=" * 60)

# æ‰§è¡Œè¯Šæ–­
data_health_check(df_dict)
```

---

## ä¸‰ã€ç´¢å¼•å¤§å¸ˆè¯¾ï¼š`loc` vs `iloc` çš„æ·±åº¦å¯¹å†³

### 3.1 æ ¸å¿ƒåŒºåˆ«ï¼šæ ‡ç­¾ç´¢å¼• vs ä½ç½®ç´¢å¼•

| ç‰¹æ€§         | `loc` (Label-based)            | `iloc` (Position-based)        |
| ------------ | ------------------------------ | ------------------------------ |
| **ç´¢å¼•ç±»å‹** | æ ‡ç­¾ï¼ˆå­—ç¬¦ä¸²ã€æ•´æ•°ç­‰ï¼‰         | æ•´æ•°ä½ç½®ï¼ˆ0-basedï¼‰            |
| **åŒ…å«æœ«ç«¯** | åŒ…å«ç»“æŸæ ‡ç­¾                   | ä¸åŒ…å«ç»“æŸä½ç½®                 |
| **ç´¢å¼•å™¨**   | å•ä¸€æ ‡ç­¾ã€åˆ—è¡¨ã€åˆ‡ç‰‡ã€å¸ƒå°”æ•°ç»„ | æ•´æ•°ã€æ•´æ•°åˆ—è¡¨ã€åˆ‡ç‰‡ã€å¸ƒå°”æ•°ç»„ |
| **æ ¸å¿ƒç”¨é€”** | ä¸šåŠ¡é€»è¾‘æŸ¥è¯¢ï¼ˆæŒ‰å§“åã€æ—¥æœŸç­‰ï¼‰ | æ•°æ®æ“ä½œï¼ˆå‰Nè¡Œã€é‡‡æ ·ç­‰ï¼‰      |

### 3.2 å®æˆ˜å¯¹æ¯”ï¼šæ˜“é”™åœºæ™¯å…¨è§£æ

```python
# æ„å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†
df_sales = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=10, freq='D'),
    'product_id': [101, 102, 103, 101, 104, 102, 101, 103, 104, 105],
    'sales': [150, 220, 180, 90, 310, 420, 130, 280, 190, 240],
    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South']
})
df_sales.set_index('date', inplace=True)  # è®¾ç½®æ—¥æœŸä¸ºç´¢å¼•
print("æ•°æ®é›†é¢„è§ˆ:")
print(df_sales)
print(f"\nç´¢å¼•ç±»å‹: {type(df_sales.index)}")

# ========== åœºæ™¯1ï¼šå•å…ƒç´ è®¿é—® ==========
print("\nğŸ” åœºæ™¯1ï¼šè·å–ç‰¹å®šæ—¥æœŸçš„é”€å”®æ•°æ®")
# locï¼šä½¿ç”¨æ—¥æœŸæ ‡ç­¾
print(f"loc['2024-01-03']:\n{df_sales.loc['2024-01-03']}")
# ilocï¼šä½¿ç”¨ä½ç½®ï¼ˆç¬¬2è¡Œï¼Œå› ä¸º0-basedï¼‰
print(f"iloc[2]:\n{df_sales.iloc[2]}")

# ========== åœºæ™¯2ï¼šåˆ‡ç‰‡æ“ä½œï¼ˆå…³é”®å·®å¼‚ï¼ï¼‰ ==========
print("\nğŸ” åœºæ™¯2ï¼šè·å–1æœˆ1æ—¥åˆ°1æœˆ5æ—¥çš„æ•°æ®")
# locï¼šåŒ…å«ç»“æŸæ ‡ç­¾
print("ä½¿ç”¨locåˆ‡ç‰‡ï¼ˆåŒ…å«æœ«ç«¯ï¼‰:")
print(df_sales.loc['2024-01-01':'2024-01-05'])  # 5è¡Œæ•°æ®

# ilocï¼šä¸åŒ…å«ç»“æŸä½ç½®
print("\nä½¿ç”¨ilocåˆ‡ç‰‡ï¼ˆä¸åŒ…å«æœ«ç«¯ï¼‰:")
print(df_sales.iloc[0:5])  # 0,1,2,3,4 â†’ 5è¡Œæ•°æ®
print(f"æ³¨æ„ï¼šiloc[0:5] è·å–çš„æ˜¯å‰5è¡Œï¼Œè€Œloc['2024-01-01':'2024-01-05']æ˜¯1-5æ—¥å…±5è¡Œ")

# ========== åœºæ™¯3ï¼šæ··åˆç´¢å¼•ï¼ˆè¡Œæ ‡ç­¾+åˆ—ä½ç½®ï¼‰ ==========
print("\nğŸ” åœºæ™¯3ï¼šè·å–ç‰¹å®šæ—¥æœŸçš„ç¬¬1ã€3åˆ—")
# é”™è¯¯å°è¯•ï¼šä¼šå¼•å‘å¼‚å¸¸
try:
    df_sales.loc['2024-01-03', [0, 2]]
except Exception as e:
    print(f"é”™è¯¯ï¼š{type(e).__name__}: {e}")

# æ­£ç¡®åšæ³•ï¼šlocéœ€è¦åˆ—æ ‡ç­¾ï¼Œilocéœ€è¦åˆ—ä½ç½®
print("\næ­£ç¡®åšæ³• - æ–¹æ³•Aï¼ˆä½¿ç”¨åˆ—åï¼‰:")
print(df_sales.loc['2024-01-03', ['product_id', 'region']])

print("\næ­£ç¡®åšæ³• - æ–¹æ³•Bï¼ˆä½¿ç”¨åˆ—ä½ç½®ï¼‰:")
print(df_sales.iloc[2, [0, 2]])  # ç¬¬2è¡Œï¼Œç¬¬0å’Œ2åˆ—

# ========== åœºæ™¯4ï¼šå¸ƒå°”ç´¢å¼• ==========
print("\nğŸ” åœºæ™¯4ï¼šæŸ¥æ‰¾é”€å”®é¢å¤§äº200çš„è®°å½•")
high_sales_mask = df_sales['sales'] > 200

# locï¼šä½¿ç”¨å¸ƒå°”æ•°ç»„
print("ä½¿ç”¨loc + å¸ƒå°”æ•°ç»„:")
print(df_sales.loc[high_sales_mask])

# ilocï¼šåŒæ ·å¯ä»¥ï¼Œä½†éœ€è¦å°†å¸ƒå°”æ•°ç»„ä¸ä½ç½®å¯¹åº”
print("\nä½¿ç”¨iloc + å¸ƒå°”æ•°ç»„:")
print(df_sales.iloc[high_sales_mask.values])

# ========== åœºæ™¯5ï¼šä¿®æ”¹æ•°æ®ï¼ˆé¿å…SettingWithCopyWarningï¼‰ ==========
print("\nğŸ” åœºæ™¯5ï¼šå®‰å…¨åœ°ä¿®æ”¹æ•°æ®")
# å±é™©ï¼šé“¾å¼ç´¢å¼•ï¼ˆå¯èƒ½å¼•å‘è­¦å‘Šæˆ–ä¸å¯é¢„æµ‹è¡Œä¸ºï¼‰
df_sales[df_sales['region'] == 'North']['sales'] = 999  # ä¸æ¨èï¼

# æ­£ç¡®ï¼šä½¿ç”¨locè¿›è¡Œæ¡ä»¶èµ‹å€¼
df_sales.loc[df_sales['region'] == 'North', 'sales'] = 888
print("ä½¿ç”¨locå®‰å…¨ä¿®æ”¹åçš„Northåœ°åŒºé”€å”®æ•°æ®:")
print(df_sales[df_sales['region'] == 'North'])

# ========== åœºæ™¯6ï¼šå¤šå±‚ç´¢å¼•ï¼ˆé«˜çº§ï¼‰ ==========
print("\nğŸ” åœºæ™¯6ï¼šå¤šå±‚ç´¢å¼•åœºæ™¯")
# åˆ›å»ºå¤šå±‚ç´¢å¼•DataFrame
df_multi = df_sales.reset_index().set_index(['region', 'date'])
df_multi.sort_index(inplace=True)
print("å¤šå±‚ç´¢å¼•DataFrame:")
print(df_multi)

# locçš„å¤šå±‚ç´¢å¼•æŸ¥è¯¢
print("\næŸ¥è¯¢Northåœ°åŒºæ‰€æœ‰æ•°æ®:")
print(df_multi.loc['North'])

print("\næŸ¥è¯¢Northåœ°åŒº1æœˆ1æ—¥åˆ°1æœˆ3æ—¥æ•°æ®:")
print(df_multi.loc[('North', slice('2024-01-01', '2024-01-03')), :])
```

### 3.3 æ€§èƒ½è€ƒé‡ï¼šä½•æ—¶ç”¨`loc`ï¼Œä½•æ—¶ç”¨`iloc`ï¼Ÿ

```python
# æ€§èƒ½æµ‹è¯•ï¼šå¤§æ•°æ®é‡ä¸‹çš„ç´¢å¼•æ€§èƒ½
large_df = pd.DataFrame(np.random.randn(1_000_000, 5), 
                       columns=['A', 'B', 'C', 'D', 'E'],
                       index=pd.date_range('2020-01-01', periods=1_000_000, freq='T'))

# åœºæ™¯Aï¼šå·²çŸ¥ä½ç½®çš„é«˜é¢‘è®¿é—® â†’ ilocæ›´å¿«
import time

# ilocï¼šç›´æ¥ä½ç½®è®¿é—®
start = time.time()
for _ in range(1000):
    _ = large_df.iloc[500000]
iloc_time = time.time() - start

# locï¼šéœ€è¦å“ˆå¸ŒæŸ¥æ‰¾
start = time.time()
target_date = large_df.index[500000]
for _ in range(1000):
    _ = large_df.loc[target_date]
loc_time = time.time() - start

print(f"å•è¡Œè®¿é—®æ€§èƒ½å¯¹æ¯”:")
print(f"  iloc: {iloc_time:.4f}ç§’ (ç›´æ¥å†…å­˜åç§»)")
print(f"  loc:  {loc_time:.4f}ç§’ (å“ˆå¸ŒæŸ¥æ‰¾ + å†…å­˜åç§»)")
print(f"  æ€§èƒ½å·®å¼‚: {loc_time/iloc_time:.1f}å€")

# æœ€ä½³å®è·µæ€»ç»“ï¼š
# 1. ä¸šåŠ¡æŸ¥è¯¢ï¼ˆæŒ‰æ—¥æœŸã€IDç­‰ï¼‰ â†’ ä½¿ç”¨loc
# 2. æ•°æ®æ“ä½œï¼ˆå‰Nè¡Œã€éšæœºé‡‡æ ·ç­‰ï¼‰ â†’ ä½¿ç”¨iloc
# 3. å¾ªç¯ä¸­é¿å…é‡å¤ç´¢å¼•è®¡ç®— â†’ å…ˆè·å–ilocä½ç½®
```

---

## å››ã€æ•°æ®æ¸…æ´—å®æˆ˜ï¼šç¼ºå¤±å€¼å¤„ç†çš„ç­–ç•¥ä¸é™·é˜±

### 4.1 ç³»ç»ŸåŒ–ç¼ºå¤±å€¼è¯Šæ–­

```python
# åˆ›å»ºå¸¦æœ‰å¤æ‚ç¼ºå¤±æ¨¡å¼çš„æ•°æ®é›†
df_with_nulls = pd.DataFrame({
    'customer_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008],
    'age': [25, 32, np.nan, 41, 29, np.nan, 35, 28],
    'income': [50000, 62000, 48000, np.nan, 53000, 71000, np.nan, 59000],
    'purchase_amount': [120.5, np.nan, 89.9, 210.3, 150.0, np.nan, np.nan, 95.7],
    'last_purchase': pd.to_datetime(['2023-12-01', '2024-01-15', np.nan, 
                                     '2024-02-20', '2024-01-30', np.nan, 
                                     '2023-11-10', '2024-02-28']),
    'segment': ['A', 'B', 'A', np.nan, 'C', 'B', 'A', 'C']
})

print("åŸå§‹æ•°æ®ï¼ˆæ˜¾ç¤ºç¼ºå¤±å€¼ï¼‰:")
print(df_with_nulls)
print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(df_with_nulls.isnull().sum())
print("\nç¼ºå¤±å€¼ç™¾åˆ†æ¯”:")
print((df_with_nulls.isnull().sum() / len(df_with_nulls) * 100).round(2))

# å¯è§†åŒ–ç¼ºå¤±æ¨¡å¼
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.heatmap(df_with_nulls.isnull(), cbar=False, cmap='viridis', 
            yticklabels=False, cbar_kws={'label': 'Missing'})
plt.title('ç¼ºå¤±å€¼åˆ†å¸ƒçƒ­å›¾', fontsize=14, pad=20)
plt.tight_layout()
plt.show()

# è¯†åˆ«ç¼ºå¤±æ¨¡å¼ï¼šMCARã€MARã€MNARï¼Ÿ
print("\nç¼ºå¤±æ¨¡å¼åˆ†æ:")
# æ£€æŸ¥ç¼ºå¤±æ˜¯å¦éšæœº
for col in df_with_nulls.columns:
    if df_with_nulls[col].isnull().any():
        # è®¡ç®—å…¶ä»–åˆ—åœ¨ç¼ºå¤±è¡Œå’Œéç¼ºå¤±è¡Œçš„å‡å€¼å·®å¼‚
        for other_col in df_with_nulls.columns:
            if other_col != col and df_with_nulls[other_col].dtype in ['int64', 'float64']:
                mean_when_null = df_with_nulls.loc[df_with_nulls[col].isnull(), other_col].mean()
                mean_when_not_null = df_with_nulls.loc[~df_with_nulls[col].isnull(), other_col].mean()
                diff_pct = abs(mean_when_null - mean_when_not_null) / mean_when_not_null * 100
                if diff_pct > 20:  # å·®å¼‚è¶…è¿‡20%
                    print(f"  {col}ç¼ºå¤±æ—¶ï¼Œ{other_col}å‡å€¼å·®å¼‚: {diff_pct:.1f}% (å¯èƒ½ééšæœºç¼ºå¤±)")
```

### 4.2 ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥çŸ©é˜µ

```python
def handle_missing_data(df, strategy='auto'):
    """
    æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†
    ç­–ç•¥é€‰é¡¹: 'auto', 'delete', 'mean', 'median', 'mode', 'forward', 'interpolate'
    """
    df_clean = df.copy()

    for col in df_clean.columns:
        if df_clean[col].isnull().sum() == 0:
            continue
        
        null_pct = df_clean[col].isnull().sum() / len(df_clean)
        dtype = df_clean[col].dtype
    
        print(f"\nå¤„ç†åˆ—: {col} (ç¼ºå¤±ç‡: {null_pct:.1%}, ç±»å‹: {dtype})")
    
        # ç­–ç•¥é€‰æ‹©é€»è¾‘
        if strategy == 'auto':
            # è‡ªåŠ¨ç­–ç•¥é€‰æ‹©
            if null_pct > 0.3:  # ç¼ºå¤±è¿‡å¤š
                current_strategy = 'delete'
            elif dtype in ['int64', 'float64']:
                # æ•°å€¼å‹ï¼šæ£€æŸ¥ååº¦
                skewness = df_clean[col].skew()
                if abs(skewness) > 1:  # åæ€åˆ†å¸ƒ
                    current_strategy = 'median'
                else:  # è¿‘ä¼¼æ­£æ€
                    current_strategy = 'mean'
            elif dtype == 'object':
                current_strategy = 'mode'
            elif pd.api.types.is_datetime64_any_dtype(df_clean[col]):
                current_strategy = 'forward'
            else:
                current_strategy = 'delete'
        else:
            current_strategy = strategy
    
        # æ‰§è¡Œå¤„ç†
        if current_strategy == 'delete':
            if null_pct > 0.5:
                print(f"  â†’ åˆ é™¤åˆ—ï¼ˆç¼ºå¤±è¿‡å¤šï¼‰")
                df_clean.drop(columns=[col], inplace=True)
            else:
                print(f"  â†’ åˆ é™¤è¡Œï¼ˆ{df_clean[col].isnull().sum()}è¡Œï¼‰")
                df_clean = df_clean.dropna(subset=[col])
    
        elif current_strategy == 'mean':
            fill_value = df_clean[col].mean()
            print(f"  â†’ å‡å€¼å¡«å……: {fill_value:.2f}")
            df_clean[col] = df_clean[col].fillna(fill_value)
    
        elif current_strategy == 'median':
            fill_value = df_clean[col].median()
            print(f"  â†’ ä¸­ä½æ•°å¡«å……: {fill_value:.2f}")
            df_clean[col] = df_clean[col].fillna(fill_value)
    
        elif current_strategy == 'mode':
            fill_value = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'
            print(f"  â†’ ä¼—æ•°å¡«å……: {fill_value}")
            df_clean[col] = df_clean[col].fillna(fill_value)
    
        elif current_strategy == 'forward':
            print(f"  â†’ å‰å‘å¡«å……")
            df_clean[col] = df_clean[col].fillna(method='ffill')
    
        elif current_strategy == 'interpolate':
            print(f"  â†’ çº¿æ€§æ’å€¼")
            df_clean[col] = df_clean[col].interpolate()

    print(f"\nå¤„ç†å®Œæˆã€‚åŸå§‹å½¢çŠ¶: {df.shape}, æ¸…æ´—åå½¢çŠ¶: {df_clean.shape}")
    return df_clean

# æ‰§è¡Œæ™ºèƒ½æ¸…æ´—
df_cleaned = handle_missing_data(df_with_nulls, strategy='auto')
print("\næ¸…æ´—åæ•°æ®:")
print(df_cleaned)
```

### 4.3 é«˜çº§æŠ€å·§ï¼šåŸºäºæ¨¡å‹çš„ç¼ºå¤±å€¼å¡«å……

```python
# ä½¿ç”¨KNNæˆ–éšæœºæ£®æ—è¿›è¡Œæ™ºèƒ½å¡«å……ï¼ˆé€‚ç”¨äºå¤æ‚æ¨¡å¼ï¼‰
from sklearn.impute import KNNImputer

# åªå¯¹æ•°å€¼åˆ—è¿›è¡ŒKNNå¡«å……
numeric_cols = df_with_nulls.select_dtypes(include=[np.number]).columns
df_numeric = df_with_nulls[numeric_cols].copy()

# æ ‡å‡†åŒ–ï¼ˆKNNå¯¹å°ºåº¦æ•æ„Ÿï¼‰
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_numeric)

# KNNå¡«å……
imputer = KNNImputer(n_neighbors=3)
df_imputed = imputer.fit_transform(df_scaled)

# åæ ‡å‡†åŒ–
df_imputed_original = scaler.inverse_transform(df_imputed)
df_final = pd.DataFrame(df_imputed_original, columns=numeric_cols, index=df_with_nulls.index)

print("KNNå¡«å……ç»“æœï¼ˆæ•°å€¼åˆ—ï¼‰:")
print(df_final)
```

---

## äº”ã€æ•°æ®IOï¼šé«˜æ€§èƒ½è¯»å†™å®æˆ˜æŒ‡å—

### 5.1 è¯»å–å¤§å‹æ–‡ä»¶çš„æ­£ç¡®å§¿åŠ¿

```python
# åœºæ™¯ï¼šå¤„ç†5GBçš„CSVæ–‡ä»¶
import os

# æŠ€å·§1ï¼šæŒ‡å®šæ•°æ®ç±»å‹ï¼Œå‡å°‘å†…å­˜å ç”¨
dtype_spec = {
    'user_id': 'int32',      # åŸæœ¬å¯èƒ½æ˜¯int64ï¼ŒèŠ‚çœ50%å†…å­˜
    'product_id': 'int32',
    'price': 'float32',      # åŸæœ¬å¯èƒ½æ˜¯float64
    'category': 'category',  # åˆ†ç±»æ•°æ®ä½¿ç”¨categoryç±»å‹
    'timestamp': 'str'       # å…ˆä»¥å­—ç¬¦ä¸²è¯»å…¥ï¼Œå†è½¬æ¢
}

# æŠ€å·§2ï¼šåªè¯»å–éœ€è¦çš„åˆ—
usecols = ['user_id', 'product_id', 'price', 'timestamp', 'is_purchased']

# æŠ€å·§3ï¼šåˆ†å—è¯»å–ï¼ˆçœŸæ­£çš„æµå¼å¤„ç†ï¼‰
chunksize = 100_000  # æ¯æ¬¡è¯»å–10ä¸‡è¡Œ
chunks = []

# å‡è®¾æ–‡ä»¶è·¯å¾„
csv_path = 'large_sales_data.csv'

# åˆ†å—è¯»å–å¹¶å¤„ç†
for i, chunk in enumerate(pd.read_csv(csv_path, 
                                       dtype=dtype_spec,
                                       usecols=usecols,
                                       chunksize=chunksize)):
    # åœ¨å†…å­˜ä¸­è¿›è¡Œå¤„ç†
    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])
    chunk['month'] = chunk['timestamp'].dt.month

    # èšåˆæˆ–è¿‡æ»¤
    monthly_summary = chunk.groupby('month')['price'].sum()

    chunks.append(monthly_summary)

    if i % 10 == 0:
        print(f"å·²å¤„ç† {(i+1)*chunksize:,} è¡Œæ•°æ®")

    # å¯é€‰ï¼šè¾¾åˆ°ä¸€å®šé‡åä¿å­˜ä¸­é—´ç»“æœ
    if i == 50:
        print("ä¿å­˜ä¸­é—´ç»“æœ...")
        # pd.concat(chunks).to_parquet('intermediate_result.parquet')

# åˆå¹¶ç»“æœ
final_result = pd.concat(chunks, axis=1).sum(axis=1)
print(f"\næœ€ç»ˆæœˆåº¦æ±‡æ€»:\n{final_result}")

# æŠ€å·§4ï¼šä½¿ç”¨é«˜æ•ˆæ ¼å¼å­˜å‚¨
print("\nä¸åŒæ ¼å¼çš„æ€§èƒ½å¯¹æ¯”:")
test_data = pd.DataFrame(np.random.randn(1_000_000, 10), 
                         columns=[f'col_{i}' for i in range(10)])

# æµ‹è¯•CSV
csv_time = %timeit -o -r 3 test_data.to_csv('test.csv', index=False)
print(f"CSVå†™å…¥: {csv_time.average:.2f}ç§’")

# æµ‹è¯•Parquetï¼ˆç°ä»£åˆ—å¼å­˜å‚¨ï¼‰
parquet_time = %timeit -o -r 3 test_data.to_parquet('test.parquet', index=False)
print(f"Parquetå†™å…¥: {parquet_time.average:.2f}ç§’ ({csv_time.average/parquet_time.average:.1f}å€æ›´å¿«)")

# è¯»å–å¯¹æ¯”
csv_read_time = %timeit -o -r 3 pd.read_csv('test.csv')
print(f"CSVè¯»å–: {csv_read_time.average:.2f}ç§’")

parquet_read_time = %timeit -o -r 3 pd.read_parquet('test.parquet')
print(f"Parquetè¯»å–: {parquet_read_time.average:.2f}ç§’ ({csv_read_time.average/parquet_read_time.average:.1f}å€æ›´å¿«)")

# æ–‡ä»¶å¤§å°å¯¹æ¯”
csv_size = os.path.getsize('test.csv') / 1024**2
parquet_size = os.path.getsize('test.parquet') / 1024**2
print(f"æ–‡ä»¶å¤§å°: CSV={csv_size:.1f}MB, Parquet={parquet_size:.1f}MB ({csv_size/parquet_size:.1f}å€å‹ç¼©)")
```

---

## å…­ã€ç»Ÿè®¡åˆ†æè¿›é˜¶ï¼šæè¿°æ€§ç»Ÿè®¡ä¸èšåˆæ¨¡å¼

### 6.1 å…¨é¢çš„æè¿°æ€§ç»Ÿè®¡æ¡†æ¶

```python
# åˆ›å»ºå¤æ‚æ•°æ®é›†
np.random.seed(42)
df_stats = pd.DataFrame({
    'department': np.random.choice(['Sales', 'Marketing', 'Engineering', 'HR'], 1000),
    'salary': np.random.lognormal(mean=10.5, sigma=0.3, size=1000).round(2),
    'bonus': np.random.exponential(scale=5000, size=1000).round(2),
    'years_experience': np.random.randint(1, 20, 1000),
    'performance_score': np.random.beta(a=2, b=5, size=1000) * 100,
    'is_manager': np.random.choice([0, 1], 1000, p=[0.8, 0.2])
})

print("æ•°æ®é›†æè¿°æ€§ç»Ÿè®¡:")
print(df_stats.describe())

# æ‰©å±•ç»Ÿè®¡ï¼šååº¦ã€å³°åº¦ã€åˆ†ä½æ•°
def extended_describe(df):
    """å¢å¼ºç‰ˆæè¿°æ€§ç»Ÿè®¡"""
    stats_df = pd.DataFrame(index=df.select_dtypes(include=[np.number]).columns)

    # åŸºç¡€ç»Ÿè®¡
    stats_df['count'] = df.count()
    stats_df['mean'] = df.mean()
    stats_df['std'] = df.std()
    stats_df['min'] = df.min()
    stats_df['25%'] = df.quantile(0.25)
    stats_df['median'] = df.median()
    stats_df['75%'] = df.quantile(0.75)
    stats_df['max'] = df.max()
    stats_df['range'] = df.max() - df.min()

    # é«˜çº§ç»Ÿè®¡
    stats_df['skewness'] = df.skew()  # ååº¦
    stats_df['kurtosis'] = df.kurt()  # å³°åº¦
    stats_df['cv'] = (df.std() / df.mean() * 100).round(2)  # å˜å¼‚ç³»æ•°
    stats_df['iqr'] = df.quantile(0.75) - df.quantile(0.25)  # å››åˆ†ä½è·
    stats_df['mad'] = df.mad()  # å¹³å‡ç»å¯¹åå·®

    # ç¼ºå¤±å€¼
    stats_df['missing'] = df.isnull().sum()
    stats_df['missing_pct'] = (df.isnull().sum() / len(df) * 100).round(2)

    # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆåŸºäºIQRï¼‰
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()
    stats_df['outliers'] = outliers
    stats_df['outliers_pct'] = (outliers / len(df) * 100).round(2)

    return stats_df.T  # è½¬ç½®ä»¥æ›´å¥½æ˜¾ç¤º

print("\nå¢å¼ºç‰ˆæè¿°æ€§ç»Ÿè®¡:")
extended_stats = extended_describe(df_stats.select_dtypes(include=[np.number]))
print(extended_stats)
```

### 6.2 é«˜æ•ˆèšåˆæ¨¡å¼ï¼šGroupByçš„è¿›é˜¶ç”¨æ³•

```python
# GroupByçš„å¤šå±‚èšåˆ
grouped = df_stats.groupby('department')

# æ–¹æ³•1ï¼šä¸€æ¬¡æ€§è®¡ç®—å¤šä¸ªèšåˆ
agg_results = grouped.agg({
    'salary': ['mean', 'median', 'std', 'min', 'max'],
    'bonus': ['sum', 'mean', lambda x: x.quantile(0.9)],  # è‡ªå®šä¹‰90åˆ†ä½æ•°
    'years_experience': 'mean',
    'performance_score': ['mean', 'count']
})

print("å¤šå±‚èšåˆç»“æœ:")
print(agg_results)

# æ–¹æ³•2ï¼šå‘½åèšåˆï¼ˆPandas 0.25+ï¼‰
agg_named = grouped.agg(
    avg_salary=('salary', 'mean'),
    median_salary=('salary', 'median'),
    total_bonus=('bonus', 'sum'),
    top10_bonus=('bonus', lambda x: x.quantile(0.9)),
    exp_years_mean=('years_experience', 'mean'),
    high_performers=('performance_score', lambda x: (x > 80).sum())
)

print("\nå‘½åèšåˆç»“æœ:")
print(agg_named)

# æ–¹æ³•3ï¼štransformï¼ˆä¿æŒåŸå§‹å½¢çŠ¶ï¼‰
# è®¡ç®—æ¯ä¸ªéƒ¨é—¨ç›¸å¯¹äºéƒ¨é—¨å¹³å‡çš„z-score
df_stats['salary_zscore'] = grouped['salary'].transform(
    lambda x: (x - x.mean()) / x.std()
)

# è®¡ç®—éƒ¨é—¨æ’å
df_stats['dept_salary_rank'] = grouped['salary'].rank(ascending=False, method='dense')

print("\nTransformæ“ä½œåçš„æ•°æ®ï¼ˆå‰10è¡Œï¼‰:")
print(df_stats[['department', 'salary', 'salary_zscore', 'dept_salary_rank']].head(10))

# æ–¹æ³•4ï¼šfilterï¼ˆåŸºäºç»„ç‰¹å¾è¿‡æ»¤ï¼‰
# åªä¿ç•™éƒ¨é—¨å¹³å‡ç»©æ•ˆå¤§äº60çš„éƒ¨é—¨æˆå‘˜
high_perf_depts = grouped.filter(lambda x: x['performance_score'].mean() > 60)
print(f"\né«˜ç»©æ•ˆéƒ¨é—¨æˆå‘˜æ•°: {len(high_perf_depts)} (åŸå§‹: {len(df_stats)})")
```
